{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyGt_ra8LSSz"
   },
   "source": [
    "## 1. 텍스트의 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YlUBrbvrLSS0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원 문장 : 목욕탕 가서 피로를 싹 풀고 집에서 자고 싶다.\n",
      "토큰화 된 문장 ['목욕탕', '가서', '피로를', '싹', '풀고', '집에서', '자고', '싶다']\n"
     ]
    }
   ],
   "source": [
    "# Token -> 텍스트를 나누는 기준.\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Flatten,Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from numpy import array\n",
    "\n",
    "# 케라스의 텍스트 전처리와 관련한 함수 중 text_to_word_sequence 함수를 불러옵니다.\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# 전처리할 텍스트를 정합니다.\n",
    "text = \"목욕탕 가서 피로를 싹 풀고 집에서 자고 싶다.\"\n",
    "\n",
    "# 해당 텍스트를 토큰화합니다.\n",
    "result = text_to_word_sequence(text)\n",
    "\n",
    "print(f\"원 문장 : {text}\")\n",
    "print(f\"토큰화 된 문장 {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2FG1NYrULSS2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " word count \n",
      " OrderedDict({'따뜻한': 1, '차': 1, '한': 1, '잔': 1, '마시며': 1, '소파에': 1, '눕고': 1, '싶다': 3, '시원한': 1, '바람': 1, '맞으며': 1, '한강변을': 1, '걷고': 1, '맛있는': 1, '저녁': 1, '먹고': 1, '아무': 1, '생각': 1, '없이': 1, '멍': 1, '때리고': 1})\n",
      "\n",
      " sentence count \n",
      " 3\n",
      "\n",
      " how many sentences each word \n",
      " defaultdict(<class 'int'>, {'소파에': 1, '잔': 1, '한': 1, '싶다': 3, '눕고': 1, '마시며': 1, '따뜻한': 1, '차': 1, '한강변을': 1, '걷고': 1, '바람': 1, '맞으며': 1, '시원한': 1, '때리고': 1, '저녁': 1, '먹고': 1, '없이': 1, '생각': 1, '아무': 1, '맛있는': 1, '멍': 1})\n",
      "\n",
      " assigned index values each word \n",
      " {'싶다': 1, '따뜻한': 2, '차': 3, '한': 4, '잔': 5, '마시며': 6, '소파에': 7, '눕고': 8, '시원한': 9, '바람': 10, '맞으며': 11, '한강변을': 12, '걷고': 13, '맛있는': 14, '저녁': 15, '먹고': 16, '아무': 17, '생각': 18, '없이': 19, '멍': 20, '때리고': 21}\n"
     ]
    }
   ],
   "source": [
    "# 단어 빈도수 세기\n",
    "\n",
    "# 전처리하려는 세 개의 문장을 정합니다.\n",
    "docs = [ \"따뜻한 차 한 잔 마시며 소파에 눕고 싶다.\",\n",
    " \"시원한 바람 맞으며 한강변을 걷고 싶다.\",\n",
    " \"맛있는 저녁 먹고 아무 생각 없이 멍 때리고 싶다.\"]\n",
    "\n",
    "# 토큰화 함수를 이용해 전처리 하는 과정입니다.\n",
    "token = Tokenizer()                               # 토큰화 함수 지정\n",
    "token.fit_on_texts(docs)       # 토큰화 함수에 문장 적용\n",
    "\n",
    "# 단어의 빈도수를 계산한 결과를 각 옵션에 맞추어 출력합니다.\n",
    "# Tokenizer()의 word_counts 함수는 순서를 기억하는 OrderedDict 클래스를 사용합니다.\n",
    "print(\"\\n word count \\n\", token.word_counts)\n",
    "# 출력되는 순서는 랜덤입니다.\n",
    "print(\"\\n sentence count \\n\", token.document_count)\n",
    "\n",
    "print(\"\\n how many sentences each word \\n\",token.word_docs)\n",
    "\n",
    "print(\"\\n assigned index values each word \\n\", token.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ko1zpzYRLSS3"
   },
   "source": [
    "## 2. 단어의 원-핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "P6pCcsCSLSS3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'운영하는': 1, '가게를': 2, '것은': 3, '사람을': 4, '것곽': 5, '같다': 6}\n"
     ]
    }
   ],
   "source": [
    "text= \"가게를 운영하는 것은 사람을 운영하는 것곽 같다.\"\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts([text])\n",
    "print(token.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "F7EGiatxLSS4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 1, 3, 4, 1, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "x=token.texts_to_sequences([text])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PKxggXb9LSS4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "# 인덱스 수에 하나를 추가해서 원-핫 인코딩 배열 만들기\n",
    "word_size = len(token.word_index) + 1\n",
    "x = to_categorical(x, num_classes=word_size)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyovn0KqLSS5"
   },
   "source": [
    "## 4.텍스트를 읽고 긍정, 부정 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1P3-i12hLSS6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'돋보였습니다': 1, '영화입니다': 2, '감동적인': 3, '스토리가': 4, '긴장감': 5, '넘치는': 6, '전개와': 7, '훌륭한': 8, '연기가': 9, '스토리는': 10, '감동적이지만': 11, '결말이': 12, '아쉬웠어요': 13, '영상미와': 14, '음악이': 15, '완벽하게': 16, '어우러진': 17, '재미있고': 18, '기억에': 19, '남아요': 20, '연출은': 21, '좋았지만': 22, '전개가': 23, '조금': 24, '지루했습니다': 25, '웃음과': 26, '감동이': 27, '적절히': 28, '섞인': 29, '가족': 30, '캐릭터': 31, '매력이': 32, '부족해': 33, '몰입하기': 34, '어려웠어요': 35, '실화를': 36, '바탕으로': 37, '연출이': 38, '전작에': 39, '비해': 40, '약해': 41, '아쉽습니다': 42, '짧지만': 43, '강렬한': 44, '인상을': 45, '남긴': 46, '작품이에요': 47}\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 리뷰 자료를 지정합니다.\n",
    "docs = [\"긴장감 넘치는 전개와 훌륭한 연기가 돋보였습니다.\", \"스토리는 감동적이지만 결말이 아쉬웠어요.\", \"영상미와 음악이 완벽하게 어우러진 영화입니다.\",\n",
    "        \"재미있고 감동적인 스토리가 기억에 남아요.\", \"연출은 좋았지만 전개가 조금 지루했습니다.\", \"웃음과 감동이 적절히 섞인 가족 영화입니다.\",\n",
    "        \"캐릭터 매력이 부족해 몰입하기 어려웠어요.\", \"실화를 바탕으로 감동적인 연출이 돋보였습니다.\", \"전작에 비해 스토리가 약해 아쉽습니다.\", \"짧지만 강렬한 인상을 남긴 작품이에요.\"]\n",
    "\n",
    "# 긍정 리뷰는 1, 부정 리뷰는 0으로 클래스를 지정합니다.\n",
    "classes = array([1,0,1,1,0,1,0,1,0,1])\n",
    "\n",
    "# 토큰화\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)\n",
    "print(token.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "P5w47DmtLSS6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " result of review text tokenizing \n",
      " [[5, 6, 7, 8, 9, 1], [10, 11, 12, 13], [14, 15, 16, 17, 2], [18, 3, 4, 19, 20], [21, 22, 23, 24, 25], [26, 27, 28, 29, 30, 2], [31, 32, 33, 34, 35], [36, 37, 3, 38, 1], [39, 40, 4, 41, 42], [43, 44, 45, 46, 47]]\n"
     ]
    }
   ],
   "source": [
    "x = token.texts_to_sequences(docs)\n",
    "print(\"\\n result of review text tokenizing \\n\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "vf3WNVZmLSS7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Result of Padding result \n",
      " [[ 6  7  8  9  1]\n",
      " [ 0 10 11 12 13]\n",
      " [14 15 16 17  2]\n",
      " [18  3  4 19 20]\n",
      " [21 22 23 24 25]\n",
      " [27 28 29 30  2]\n",
      " [31 32 33 34 35]\n",
      " [36 37  3 38  1]\n",
      " [39 40  4 41 42]\n",
      " [43 44 45 46 47]]\n"
     ]
    }
   ],
   "source": [
    "# 패딩, 서로 다른 길이의 데이터를 4로 맞추어 줍니다.\n",
    "padded_x = pad_sequences(x, 5)\n",
    "print(\"\\n Result of Padding result \\n\", padded_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "HkhKyqFmLSS7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Embedding name=embedding_3, built=False>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임베딩에 입력될 단어의 수를 지정합니다.\n",
    "word_size = len(token.word_index) +1\n",
    "\n",
    "# 단어 임베딩을 포함하여 딥러닝 모델을 만들고 결과를 출력합니다.\n",
    "Embedding(word_size, 8, input_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "2mE5DbHrLSS8"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(word_size, 8, input_length=5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.7000 - loss: 0.6854\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7000 - loss: 0.6814\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8000 - loss: 0.6774\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8000 - loss: 0.6733\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9000 - loss: 0.6693\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9000 - loss: 0.6653\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9000 - loss: 0.6614\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.6574\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.6534\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.6494\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.6454\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.6414\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.6374\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.6334\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.6294\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.6254\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.6213\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.6173\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.6132\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.6091\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x324420bc0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(padded_x, classes, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
